---
title: 'Comparison of Conformal Prediction and Bootstrapping on a Regression Problem with Random Forests'
#subtitle: '732A76 Research Project /n Linköping University'
subtitle: '732A76 Research Project'
author: |
  | David Hrabovszki
  | Linköping University
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: my_header.tex
  html_document:
    df_print: paged
header-includes: \usepackage{float}
bibliography: bibliopaper.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r warning = FALSE, message = FALSE}
# libraries
library(knitr)
library(kableExtra)
library(tidyverse)
```

```{r}
load('results_all.RData')
```

# Introduction

Conformal prediction is a relatively new framework for obtaining prediction intervals. Its main advantage is that it guarantees that the probability of making accurate predictions is larger than or equal to a user specified confidence level. Conformal prediction can be built on an arbitrary model that results in point prediction, and it will create prediction intervals from it. Inductive conformal prediction was suggested in [@icp2005], which has a very computationally efficient way of calculating intervals by only training the underlying model once. Therefore, this paper also uses this approach.

In this experiment, conformal prediction and parametric bootstrapping algorithms are implemented and their performances are compared on a regression task, where the underlying model is a random forest and the data used is artificially generated. The experiments are run with different sizes of datasets as well. The code used can be found on https://github.com/hradave/Research_project. Standard R packages were used where possible, but the conformal prediction was implemented from scratch building on the papers [@conf2014] and [@var2016].

The competing methods are evaluated along three metrics: coverage rate, mean size of prediction regions, runtime. Coverage rate is the ratio of the test instances that fall into the prediction region, mean size of prediction regions is referred to as efficiency in [@conf2014] and is simply the average of the prediction intervals over the test set. Runtime is the CPU time needed to train the models, and to calibrate the nonconformity scores in the case of conformal prediction. Prediction times are not included in this measure.

# Methods

## Conformal Prediction

The use of conformal prediction allows us to obtain a prediction region instead of a single-point prediction, where the regions can be different sizes for each instance. It also guarantees that the probability of making an error (the test instance being outside the prediction interval) is bounded by a confidence level set by the user, this makes all conformal predictors valid [@conf2014]. Conformal prediction can be built on an arbitrary prediction model, in this paper a random forest is used for this purpose.

In this paper, similarly to the one referenced above, inductive conformal prediction is implemented, where the underlying model only has to be trained once on the training set, which can be used for prediction on the test set. In this case however, a separate dataset is required to calibrate the nonconformity scores.

The nonconformity scores are calculated on the calibration and test sets using the nonconformity function, which is usually related to the absolute error of the prediction for each instance:

$$\alpha_i = |y_i-\hat{y_i}|$$

The scores represent how different the new example is from the old ones. If it is relatively small, then that instance fits well into the model that we trained on the training set. The role of the calibration set is to find the smallest nonconformity score, compared to which, at least 95% (or other confidence level) of the calibration data points' scores are smaller. Taking this smallest score (bound) $\alpha_{s(\delta)}$ , we can create prediction regions for the test instances by adding this bound to and subtracting it from the predicted value:

$$\hat{Y}_j^{\delta}=\hat{y}_j\pm\alpha_{s(\delta)}$$

Since this method does not depend on the individual test observation, the regions will have the same size, which would cancel one of the main advantages of conformal prediction. Therefore, this paper employs a normalized nonconformity function instead, where the scores are calculated by dividing the absolute prediction error by $\sigma_i$, which is the estimated difficulty of the random forest model for making a correct prediction for instance $y_i$.

$$\alpha_i = \frac{|y_i-\hat{y_i}|}{\sigma_i}$$

The prediction intervals then become:

$$\hat{Y}_j^{\delta}=\hat{y}_j\pm\alpha_{s(\delta)}*\sigma_j$$

There are several very different ways to calculate $\sigma$, but this paper builds on the conclusions of the experiments in [@var2016], and implements the method which proved to be the least computationally expensive, but equally efficient approach, the variance-based measure. This method takes the variance of the predictions of the individual trees in the random forest as the difficulty of each instance, because it assumes that there is a larger understanding (smaller variance) between the trees, if the instance is easy to predict. The nonconformity scores are calculated this way:

$$\alpha_i = \frac{|y_i-\hat{y_i}|}{\mu_i+\beta}$$
where $\beta$ is a parameter that controls the sensitivity of the nonconformity measure and $\mu_i$ is the variance of the individual predictions for instance $i$.

## Bootstrap

Bootstrapping is a widely used algorithm for creating confidence and prediction intervals using datasets randomly sampled with replacement (non-parametric) or generated from their distributions (parametric). Since this paper experiments with artificially simulated data, so the distribution of the response value is known, the more precise parametric version can be implemented. 

A specified number (R) of training set replicates are generated, the random forest model is trained on each of them, and the predictions are made for the test test in every iteration. This results in a distribution of prediction values for every test instance, and the quantile method can be used to obtain 95% (or any other) confidence or prediction bands.

# Empirical Evaluation

## Experimental Setup

### Data Simulation

Empirical experiments are often conducted on real world datasets, but because this research focuses on the parametric version of bootstrapping, we opted for an artificially simulated dataset instead. This means that we can be certain about the distribution of the response variable, which is necessary for parametric bootstrap.
The implementation was based on a previous paper that uses an additive error model [@sim2020]: 

$Y = m(X) + \epsilon$ , where $X\sim\mathcal{N}(0,\Sigma_{50})$ and $\Sigma_{50}$ is an AR(1) covariance matrix with $\rho = 0.6$ and diagonal values equal to 1. The error term  $\epsilon$ is Gaussian noise and the $m(X)$ mean function is nonlinear with interaction between the predictors to make the data more complex. 100,000 instances were simulated from this model. The response variable is normalized, so that the resulting interval sizes can be more easily interpreted. An interval size of 0.5 means that the region covers 50% of the range the variable can take.


### Data Splits

The experiments use the holdout method for evaluation with 70% - 30% split between training and test sets. The first 70,000 instances were used as the pool for training and the last 30,000 for testing. For each different data size, the appropriate number of observations were sampled from these pools randomly to ensure that no test instance has been used for training or for validation while optimizing the parameters. It is important to note that while bootstrapping uses all training examples for training the random forest, conformal prediction utilizes only a part of it for proper training and the rest for calibration. A possible approach would have been to make the number of instances used for training equal for both methods, but since calibration is an important step in obtaining prediction intervals, I made the decision to use a subset of the training data for calibration. The number of calibration instances was set to $$q = 100 * \Bigg{\lfloor}\frac{|Z|}{400}\Bigg{\rfloor}-1$$ according to [@conf2014], where Z is the full training set. This means that around 20 - 25% of the full training set was used for calibration.

### Computer Specifications

The experiments were run on a notebook with an Intel Core i5-8250U processor (4-core 1.6 GHz base frequency) and 12GB of RAM. The conformal predictions were run on a single core, but the bootstrapping utilized all cores in parallel, which is straightforward to implement with the boot() function on a Linux system.

### Chosen Parameters

Three parameters were optimized using grid search: the number of trees in a random forest (ntree), the sensitivity of the nonconformity measure in conformal prediction ($\beta$) and the number of bootstrap replicates (R). 
10,000 instances were used from the training pool for the grid search, where **ntree** was chosen to be **125**, because this value resulted in a “low enough” mean squared error. A higher number of trees would make the predictions slightly more accurate, but only at the a significantly larger computation cost.
**$\beta$** was chosen to be **0.01**, because the mean of conformal prediction region sizes was the smallest with this value, and the runtime was slightly shorter than with others. The confidence level set for the interval was barely affected by $\beta$. The number of bootstrap replicates should be as large as possible, but increasing this parameter increases the runtime linearly, so the optimal value was set to a low value **(R = 200)** that still results in reasonably good coverage rates and mean region sizes.


## Results

Prediction intervals were constructed using conformal prediction and bootstrapping at 95% confidence level on artificially simulated datasets of sizes 1,000 - 10,000. In each run, we monitor the coverage rate, the mean interval size and the time required to train the models.

Theoretically, the coverage rate of the conformal prediction should be around the confidence level [@conf2014], because the prediction intervals produced are always valid. This is exactly what we observe here as well, the coverage rate is slightly larger than 0.95, but it seems to converge to it as the number of observations grows. Bootstrapping, however, resulted in much lower coverage rates in all the runs, and there seems to be no indication that a larger dataset would solve this problem (Figure \ref{fig:results} top left).


```{r fig.cap="\\label{fig:results} Results", out.width = "100%", fig.height = 7, fig.width = 10, fig.pos='!h', fig.align='center'}
par(mfrow=c(2,2))
plot(results_all$size, results_all$icp_covrate, type = 'l', ylim = c(0.55,1), col = 'blue', ylab = 'Coverage rate', xlab = 'Data size')
points(results_all$size, results_all$bs_covrate, type = 'l', col = 'red')
abline(h = 0.95, lty = 2)
#legend('left', legend=c('ICP', 'BS'), lty=c(1,1), col=c('blue', 'red'))

plot(results_all$size, results_all$icp_mean_interval_size, type = 'l', ylim = c(0,0.3), col = 'blue', xlab = 'Data size', ylab = 'Mean interval size')
points(results_all$size, results_all$bs_mean_interval_size, type = 'l', col = 'red')
#legend('bottom', legend=c('ICP', 'BS'), lty=c(1,1), col=c('blue', 'red'))

par(mar = c(5,4,4,2)+0.1)
with(results_all, plot(size, icp_runtime, type="l", col='blue', ylab = 'Runtime of CP (secs)', xlab = 'Data size'))
par(new = T)
with(results_all, plot(size, bs_runtime/60, type = 'l', col = 'red', ylab = NA, xlab = NA, axes = F))
axis(side = 4)
mtext(side = 4, line = 3, 'Runtime of BS (mins)')
#legend('bottom', legend=c('CP', 'BS'), lty=c(1,1), col=c('blue', 'red'))

plot(1, xaxt='n', yaxt = 'n',ann=FALSE, bty = 'n', col = 'white')
legend('center', legend=c('Conformal Prediction', 'Bootstrapping'), lty=c(1,1), col=c('blue', 'red'), cex = 1.5, bty = 'n')

```




The mean size of the prediction regions decreases in both cases as the dataset grows, but the bands resulting from bootstrapping are much narrower, around half the size of the conformal prediction bands. The prediction regions cover around 10% of the whole range with bootstrapping, and 20% - 25% with conformal prediction (Figure \ref{fig:results} top right).


There is a significant difference between the runtimes of the two methods, even though it increases linearly  in both cases as we use more data. Conformal prediction in general seems to be much faster, than bootstrapping. Conformal prediction is measured in seconds, and bootstrapping is measured in minutes on Figure \ref{fig:results} (bottom left). 


# Conclusion

The fact that prediction regions from conformal prediction are valid is no surprise, but the regions from bootstrapping are clearly worse than expected, because they do not include 95% of the test instances.

The mean sizes of the intervals are therefore much narrower with bootstrapping than with conformal prediction, but I would not draw conclusions from this, because this also means that the intervals do not have the confidence levels that we set.

The difference in runtime between the two methods is very conspicuous, conformal prediction is significanlty faster. This is fairly easy to explain, because bootstrapping trains the random forest model R (number of bootstrap replicates) times (+1 for the initial estimates), whereas conformal prediction only trains once. This is a huge advantage of the conformal method, because training the model is by far the most computationally expensive task in prediction. Bootstrapping also has to perform random data generation for every replicate. Conformal prediction has to calibrate the bound of the non-conformity scores on the calibration set, but this only includes one apply() function call, some vector operations and one sorting. The overhead costs of conformal predictions are minute relative to the cost of training the model R times, therefore this paper concludes that conformal prediction is much faster, than bootstrapping, while producing more accurate (valid) prediction intervals.

# Future Improvements

Several modifications could be made to improve this study of comparing two different methods. The parameters of the random forest and the number of bootstrap replicates could be further optimizied for different data sizes, but in this paper I intended to keep a setup unchanged for more straigthforward comparison. We could look at out-of-bag predictions of the random forests instead of using a separate set for either calibration or testing, but this would not necessarily improve predictive quality. It would be worth an experiment though, because then the available dataset would be better utilized. The data simulation process could be refined, and real world datasets could be used as well to see how the two methods compare against each other in a more real world setting.




\pagebreak

# References

::: {#refs}
:::

# Appendix

```{r}
results_all$icp_covrate = round(results_all$icp_covrate,3)
results_all$icp_mean_interval_size = round(results_all$icp_mean_interval_size,3)
results_all$icp_runtime = round(results_all$icp_runtime,1)
results_all$bs_covrate = round(results_all$bs_covrate,3)
results_all$bs_mean_interval_size = round(results_all$bs_mean_interval_size,3)
results_all$bs_runtime = round(results_all$bs_runtime,1)
colnames(results_all) = c("Size", "CP coverage rate", "CP region size", "CP runtime",
                          "BS coverage rate", "BS region size", "BS runtime")
kable(results_all, caption = "Results of experimental runs", 
      label = "results", row.names = FALSE) %>%
  kable_styling(latex_options = "hold_position")
```

